import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
import joblib
import os
from datetime import datetime

class ModelTrainer:
    def __init__(self):
        self.models = {}
        self.label_encoders = {}
        self.scalers = {}
        
    def preprocess_data(self, df, model_version="v1.0"):
        """Enhanced preprocessing for different model types"""
        df_processed = df.copy()
        
        # Handle categorical variables
        categorical_columns = ['internet_service', 'contract_type', 'payment_method']
        
        for col in categorical_columns:
            key = f"{model_version}_{col}"
            if key not in self.label_encoders:
                self.label_encoders[key] = LabelEncoder()
                df_processed[col] = self.label_encoders[key].fit_transform(df_processed[col])
            else:
                df_processed[col] = self.label_encoders[key].transform(df_processed[col])
        
        # Feature engineering (enhanced for v2.0)
        df_processed['avg_monthly_spend'] = df_processed['total_charges'] / df_processed['months_active']
        df_processed['support_per_month'] = df_processed['support_tickets'] / df_processed['months_active']
        
        # Advanced features for v2.0
        if model_version == "v2.0":
            df_processed['customer_lifetime_value'] = df_processed['monthly_charges'] * df_processed['months_active']
            df_processed['support_intensity'] = np.where(df_processed['support_tickets'] > 3, 1, 0)
            df_processed['high_value_customer'] = np.where(df_processed['monthly_charges'] > 80, 1, 0)
        
        # Feature columns
        base_features = ['age', 'months_active', 'monthly_charges', 'total_charges', 
                        'internet_service', 'contract_type', 'payment_method', 
                        'support_tickets', 'avg_monthly_spend', 'support_per_month']
        
        if model_version == "v2.0":
            feature_columns = base_features + ['customer_lifetime_value', 'support_intensity', 'high_value_customer']
        else:
            feature_columns = base_features
            
        return df_processed[feature_columns], feature_columns
    
    def train_random_forest_v1(self, df):
        """Original Random Forest model (conservative)"""
        print("Training Random Forest v1.0...")
        X, feature_columns = self.preprocess_data(df, "v1.0")
        y = df['churn']
        
        # Scale features for consistency
        scaler_key = "v1.0_scaler"
        self.scalers[scaler_key] = StandardScaler()
        X_scaled = self.scalers[scaler_key].fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_proba)
        
        print(f"Random Forest v1.0 AUC: {auc_score:.4f}")
        
        self.models["v1.0"] = {
            'model': model,
            'feature_columns': feature_columns,
            'version': "v1.0",
            'algorithm': "RandomForest",
            'auc_score': auc_score,
            'training_date': datetime.now().isoformat()
        }
        
        return X_test, y_test, y_pred
    
    def train_xgboost_v11(self, df):
        """XGBoost model (potentially more accurate)"""
        print("Training XGBoost v1.1...")
        X, feature_columns = self.preprocess_data(df, "v1.1")
        y = df['churn']
        
        # Scale features
        scaler_key = "v1.1_scaler"
        self.scalers[scaler_key] = StandardScaler()
        X_scaled = self.scalers[scaler_key].fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        model = xgb.XGBClassifier(
            n_estimators=150,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='auc'
        )
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_proba)
        
        print(f"XGBoost v1.1 AUC: {auc_score:.4f}")
        
        self.models["v1.1"] = {
            'model': model,
            'feature_columns': feature_columns,
            'version': "v1.1",
            'algorithm': "XGBoost",
            'auc_score': auc_score,
            'training_date': datetime.now().isoformat()
        }
        
        return X_test, y_test, y_pred
    
    def train_logistic_regression_v2(self, df):
        """Logistic Regression with advanced features (fast, interpretable)"""
        print("Training Logistic Regression v2.0...")
        X, feature_columns = self.preprocess_data(df, "v2.0")
        y = df['churn']
        
        # Scale features (critical for logistic regression)
        scaler_key = "v2.0_scaler"
        self.scalers[scaler_key] = StandardScaler()
        X_scaled = self.scalers[scaler_key].fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        model = LogisticRegression(random_state=42, max_iter=1000, C=0.1)
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_proba)
        
        print(f"Logistic Regression v2.0 AUC: {auc_score:.4f}")
        
        self.models["v2.0"] = {
            'model': model,
            'feature_columns': feature_columns,
            'version': "v2.0",
            'algorithm': "LogisticRegression",
            'auc_score': auc_score,
            'training_date': datetime.now().isoformat()
        }
        
        return X_test, y_test, y_pred
    
    def save_models(self):
        """Save all trained models with metadata"""
        for version, model_data in self.models.items():
            # Create version directory
            version_dir = f"models/{version}"
            os.makedirs(version_dir, exist_ok=True)
            
            # Save model with all preprocessing components
            model_package = {
                'model': model_data['model'],
                'feature_columns': model_data['feature_columns'],
                'label_encoders': {k: v for k, v in self.label_encoders.items() if k.startswith(version)},
                'scaler': self.scalers.get(f"{version}_scaler"),
                'metadata': {
                    'version': model_data['version'],
                    'algorithm': model_data['algorithm'],
                    'auc_score': model_data['auc_score'],
                    'training_date': model_data['training_date']
                }
            }
            
            filepath = f"{version_dir}/model.pkl"
            joblib.dump(model_package, filepath)
            print(f"Saved {model_data['algorithm']} {version} to {filepath}")
            
            # Save metadata separately for easy access
            metadata_path = f"{version_dir}/metadata.json"
            import json
            with open(metadata_path, 'w') as f:
                json.dump(model_package['metadata'], f, indent=2)

if __name__ == "__main__":
    # Load training data (reuse from lesson 1)
    data_path = "../lesson-01-churn-prediction/data/telecom_customers.csv"
    if not os.path.exists(data_path):
        print("Training data not found. Please run lesson 1 data generation first.")
        exit(1)
    
    data = pd.read_csv(data_path)
    print(f"Loaded {len(data)} customer records")
    
    # Train all model variants
    trainer = ModelTrainer()
    
    # Train three different model approaches
    trainer.train_random_forest_v1(data)
    trainer.train_xgboost_v11(data)  
    trainer.train_logistic_regression_v2(data)
    
    # Save all models
    trainer.save_models()
    
    print("\nðŸŽ¯ Model Training Complete!")
    print("Next step: Build the model router to serve all three models")